
print('class MultiHeadAttention(nn.Module):       def __init__(self, d_model: int, num_heads: int, dropout_rate: float):           super().__init__()           # 定义dropout以防止过拟合           self.dropout = nn.Dropout(dropout_rate)                      # 引入权重矩阵，所有参数都可学习           self.W_q = nn.Linear(d_model, d_model)           self.W_k = nn.Linear(d_model, d_model)           self.W_v = nn.Linear(d_model, d_model)           self.W_o = nn.Linear(d_model, d_model)              self.num_heads = num_heads           assert d_model % num_heads == 0, "d_model must be divisible by number of heads"                      #  d_k是每个分割的自注意力头的新维度           self.d_k = d_model // num_heads          def forward(self, q, k, v, encoder_mask=None):                      # 我们将使用多个序列的批处理来Parallel训练我们的模型，因此我们需要在形状中包括batch_size。           # query、key和value是通过将相应的权重矩阵与输入嵌入相乘来计算的。           # 形状变化：q(batch_size, seq_len, d_model) @ W_q(d_model, d_model) => query(batch_size, seq_len, d_model) [key和value的情况相同]。           query = self.W_q(q)            key = self.W_k(k)           value = self.W_v(v)              # 将query、key和value分割成多个head。d_model在8个头中分割为d_k。           # 形状变化：query(batch_size, seq_len, d_model) => query(batch_size, seq_len, num_heads, d_k) -> query(batch_size,num_heads, seq_len,d_k) [key和value的情况相同]。           query = query.view(query.shape[0], query.shape[1], self.num_heads ,self.d_k).transpose(1,2)           key = key.view(key.shape[0], key.shape[1], self.num_heads ,self.d_k).transpose(1,2)           value = value.view(value.shape[0], value.shape[1], self.num_heads ,self.d_k).transpose(1,2)              # :: SELF ATTENTION BLOCK STARTS ::              # 计算attention_score以查找query与key本身及序列中所有其他嵌入的相似性或关系。           # 形状变化：query(batch_size,num_heads, seq_len,d_k) @ key(batch_size,num_heads, seq_len,d_k) => attention_score(batch_size,num_heads, seq_len,seq_len)。           attention_score = (query @ key.transpose(-2,-1))/math.sqrt(self.d_k)              # 如果提供了mask，则需要根据mask值修改attention_score。参见第4点的详细信息。           if encoder_mask is not None:               attention_score = attention_score.masked_fill(encoder_mask==0, -1e9)                      # softmax函数计算所有attention_score的概率分布。它为较高的attention_score分配较高的概率值。这意味着更相似的令牌获得较高的概率值。           # 形状变化：与attention_score相同           attention_weight = torch.softmax(attention_score, dim=-1)              if self.dropout is not None:               attention_weight = self.dropout(attention_weight)              # 自注意力块的最后一步是，将attention_weight与值嵌入向量相乘。           # 形状变化：attention_score(batch_size,num_heads, seq_len,seq_len) @  value(batch_size,num_heads, seq_len,d_k) => attention_output(batch_size,num_heads, seq_len,d_k)           attention_output = attention_score @ value                      # :: SELF ATTENTION BLOCK ENDS ::              # 现在，所有head都将组合回一个head           # 形状变化：attention_output(batch_size,num_heads, seq_len,d_k) => attention_output(batch_size,seq_len,num_heads,d_k) => attention_output(batch_size,seq_len,d_model)               attention_output = attention_output.transpose(1,2).contiguous().view(attention_output.shape[0], -1, self.num_heads * self.d_k)              # 最后attention_output将与输出权重矩阵相乘以获得最终的多头注意力输出。           # multihead_output的形状与嵌入输入相同           # multihead_output的形状与嵌入输入相同           multihead_output = self.W_o(attention_output)                      return multihead_output   '.replace("   ", "\n"))
